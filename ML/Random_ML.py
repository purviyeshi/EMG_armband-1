import tensorflow as tf
import numpy as np
from sklearn.metrics import accuracy_score
from tensorflow.keras import layers, models, utils
from sklearn.preprocessing import MinMaxScaler

def feature_engineering(X):
    scaler = MinMaxScaler(feature_range=(0, 1))
    normalized_X = scaler.fit_transform(X)
    return normalized_X


# Number of classes
num_classes = 7 # Possible Outputs

# Training DataSets for output
X_train_output_1 = np.array([
        [0.4703154, 0.63322699, 0.69644405, 0.06736446, 0.03247704],
        [0.54235134, 0.1859426, 0.72248764, 0.53070503, 0.77257744],
        [0.32456118, 0.1663578, 0.72360184, 0.41906082, 0.86514464],
        [0.22498271, 0.83096694, 0.26871469, 0.3351098, 0.53240072],
        [0.94423135, 0.83816024, 0.97320737, 0.27220288, 0.26674214],
        [0.46825323, 0.95651176, 0.62287754, 0.55850722, 0.81712193],
        [0.71778669, 0.38773798, 0.52335327, 0.0967964, 0.98956312]
        ])

X_train_output_2 = np.array([
        [0.60089115, 0.36488145, 0.87087368, 0.33971387, 0.03802845],
        [0.61277093, 0.38993743, 0.87477833, 0.55660918, 0.89346649],
        [0.57722313, 0.21805156, 0.43641, 0.7618002, 0.84320788],
        [0.19548962, 0.93223893, 0.30566128, 0.74998702, 0.18126583],
        [0.16017154, 0.24864802, 0.77094856, 0.77483653, 0.82184255],
        [0.50639232, 0.64874628, 0.1349389, 0.14074458, 0.69916296],
        [0.6831326, 0.60399694, 0.14690748, 0.24736268, 0.01567647]
    ])

X_train_output_3 = np.array([
        [0.35963965, 0.82415635, 0.50897106, 0.17845952, 0.72131665],
        [0.61294238, 0.30834088, 0.16001852, 0.25791359, 0.62378671],
        [0.23948709, 0.26345684, 0.01481734, 0.61221879, 0.52209577],
        [0.06091749, 0.6799478, 0.93480372, 0.56271758, 0.49216786],
        [0.69435321, 0.83879284, 0.77668618, 0.1783796, 0.42646014],
        [0.07400053, 0.26726736, 0.99943507, 0.70528387, 0.49872954],
        [0.949613, 0.27302449, 0.27220583, 0.50036213, 0.67388371]
    ])

X_train_output_4 = np.array([
        [0.82799184, 0.58102133, 0.60014039, 0.1112946, 0.6225857],
        [0.94493261, 0.51506931, 0.82843994, 0.98261287, 0.86420544],
        [0.86599247, 0.39930937, 0.20936012, 0.04146218, 0.58914775],
        [0.35357549, 0.77162733, 0.55007338, 0.44129444, 0.27345774],
        [0.38268949, 0.11125123, 0.93564933, 0.42599485, 0.36710369],
        [0.75773582, 0.23941288, 0.10013984, 0.16212391, 0.562033],
        [0.94188442, 0.83737724, 0.73362798, 0.08239674, 0.53328747]
    ])

X_train_output_5 = np.array([
        [0.80449231, 0.76329198, 0.88372999, 0.76022858, 0.13407161],
        [0.48192703, 0.10604622, 0.96924844, 0.5313542, 0.91336737],
        [0.11770071, 0.45703373, 0.98470446, 0.40893153, 0.43786964],
        [0.80321042, 0.9228988, 0.30815758, 0.59191713, 0.58683427],
        [0.65449009, 0.96597841, 0.57636321, 0.57797594, 0.21224968],
        [0.62712329, 0.79668846, 0.3036114, 0.83947644, 0.92531171],
        [0.47200052, 0.93220791, 0.07955779, 0.87205877, 0.63754426]
    ])

X_train_output_6 = np.array([
        [0.67058822, 0.04008808, 0.5553372, 0.42202579, 0.8808876],
        [0.47034325, 0.65806352, 0.92979806, 0.50040429, 0.1369623],
        [0.59572961, 0.28778999, 0.92073555, 0.01155217, 0.32883633],
        [0.54597208, 0.55041603, 0.11995042, 0.22079075, 0.22221482],
        [0.09329175, 0.39671086, 0.51360281, 0.39864919, 0.94878581],
        [0.30174965, 0.76212318, 0.2303908, 0.52832687, 0.81609601],
        [0.86139999, 0.75974225, 0.45608603, 0.66085875, 0.82550862]
    ])

X_train_output_7 = np.array([
        [0.52462466, 0.63097604, 0.8534363, 0.86813143, 0.05729867],
        [0.66827074, 0.47983957, 0.79708162, 0.90943134, 0.93866499],
        [0.77990364, 0.92958197, 0.48922568, 0.50244618, 0.28034446],
        [0.19924566, 0.79080533, 0.65972408, 0.14812237, 0.6317662],
        [0.34513073, 0.75328996, 0.46164397, 0.26337902, 0.15699477],
        [0.81784628, 0.51934206, 0.2539822, 0.47095692, 0.62486823],
        [0.5137402, 0.30992864, 0.59814085, 0.84467065, 0.76942998]
    ])


# Applying feature engineering to each output
X_train_features_1 = feature_engineering(X_train_output_1)
X_train_features_2 = feature_engineering(X_train_output_2)
X_train_features_3 = feature_engineering(X_train_output_3)
X_train_features_4 = feature_engineering(X_train_output_4)
X_train_features_5 = feature_engineering(X_train_output_5)
X_train_features_6 = feature_engineering(X_train_output_6)
X_train_features_7 = feature_engineering(X_train_output_7)


y_train_output_1 = np.array([0, 0, 0, 0, 0, 0, 0])
y_train_output_2 = np.array([1, 1, 1, 1, 1, 1, 1])
y_train_output_3 = np.array([2, 2, 2, 2, 2, 2, 2])
y_train_output_4 = np.array([3, 3, 3, 3, 3, 3, 3])
y_train_output_5 = np.array([4, 4, 4, 4, 4, 4, 4])
y_train_output_6 = np.array([5, 5, 5, 5, 5, 5, 5])
y_train_output_7 = np.array([6, 6, 6, 6, 6, 6, 6])


# Convert labels to one-hot encoding for each output
y_train_one_hot_1 = utils.to_categorical(y_train_output_1, num_classes=num_classes)
y_train_one_hot_2 = utils.to_categorical(y_train_output_2, num_classes=num_classes)
y_train_one_hot_3 = utils.to_categorical(y_train_output_3, num_classes=num_classes)
y_train_one_hot_4 = utils.to_categorical(y_train_output_4, num_classes=num_classes)
y_train_one_hot_5 = utils.to_categorical(y_train_output_5, num_classes=num_classes)
y_train_one_hot_6 = utils.to_categorical(y_train_output_6, num_classes=num_classes)
y_train_one_hot_7 = utils.to_categorical(y_train_output_7, num_classes=num_classes)


# Generate X_test data in vector matrix form
X_test = np.array([
    [0.39543753, 0.11371693, 0.19780681, 0.40895655, 0.21898742],
    [0.0731897, 0.38789199, 0.14662578, 0.73530654, 0.99199097],
    [0.78214035, 0.48487785, 0.67007236, 0.69200084, 0.40665228],
    [0.49931299, 0.22369282, 0.7074965, 0.01723722, 0.49560964],
    [0.2735199, 0.966509, 0.95782868, 0.18587524, 0.04626082],
    [0.49367337, 0.65436533, 0.61792889, 0.47303204, 0.70334193],
    [0.70611917, 0.99180793, 0.91589668, 0.2160665, 0.54634508],
    [0.92393066, 0.57767, 0.69098818, 0.82989774, 0.58528089],
    [0.02089229, 0.80498648, 0.08887889, 0.13027261, 0.8600037],
    [0.86018795, 0.7717956, 0.06017976, 0.20262374, 0.13034911],
    [0.30075341, 0.22338627, 0.33860167, 0.06507698, 0.82790307],
    [0.63766445, 0.30305224, 0.08845985, 0.7710765, 0.20790253],
    [0.18936926, 0.61542359, 0.41102779, 0.14856231, 0.21347781],
    [0.29602035, 0.35136105, 0.98920269, 0.25814799, 0.72957398],
    [0.69758012, 0.24777904, 0.41610876, 0.44315299, 0.48529413],
    [0.17939777, 0.71188907, 0.20825801, 0.86808518, 0.22718222],
    [0.28974395, 0.09882792, 0.24992216, 0.35527544, 0.41331991],
    [0.08710918, 0.50910498, 0.9388592, 0.08006289, 0.63781567],
    [0.83371585, 0.87209399, 0.13247115, 0.25125093, 0.24949923],
    [0.64412024, 0.34079189, 0.03235859, 0.06756957, 0.23216538],
    [0.59399339, 0.72601797, 0.32343804, 0.54774791, 0.87752662],
    [0.12340961, 0.55311048, 0.62937098, 0.65286679, 0.5821508],
    [0.38759434, 0.33989807, 0.87002832, 0.81580919, 0.70578399],
    [0.32769506, 0.35680095, 0.2988167, 0.24991077, 0.5119039],
    [0.90593897, 0.77793919, 0.62928128, 0.75539857, 0.24418461],
    [0.00225327, 0.13198985, 0.94056133, 0.96595331, 0.82122537],
    [0.7757084, 0.70307949, 0.27428494, 0.73897998, 0.81608568],
    [0.67762952, 0.97592472, 0.20630128, 0.1125493, 0.12580357],
    [0.69215039, 0.53273164, 0.29772524, 0.31011478, 0.08814024],
    [0.94784486, 0.75793337, 0.46203452, 0.65359711, 0.89477731],
    [0.06085408, 0.3883086, 0.75558154, 0.81784917, 0.80333886],
    [0.50612793, 0.34185804, 0.5757661, 0.18517234, 0.36873696],
    [0.86272385, 0.49290745, 0.026111, 0.04600493, 0.7612001],
    [0.0158037, 0.50396772, 0.51089934, 0.43400243, 0.85791079],
    [0.71892309, 0.86056757, 0.74615329, 0.87975888, 0.43266921],
    [0.30311153, 0.58872261, 0.01169092, 0.39865801, 0.20293327],
    [0.22615344, 0.27001587, 0.28122575, 0.81875479, 0.27806361],
    [0.65389373, 0.53808602, 0.51596433, 0.07836848, 0.74565366],
    [0.80454004, 0.7469537, 0.74628647, 0.81032517, 0.60162841],
    [0.29269341, 0.07114471, 0.98230391, 0.0831862, 0.45477399],
    [0.2449036, 0.66329569, 0.84409468, 0.61454616, 0.25492716],
    [0.24430619, 0.53983109, 0.50397008, 0.73802332, 0.11259315],
    [0.58543337, 0.01328045, 0.65407003, 0.07317718, 0.09227511],
    [0.80428821, 0.51776739, 0.25853454, 0.01511691, 0.06884022],
    [0.67861864, 0.19301807, 0.87583482, 0.07122706, 0.91953562],
    [0.62234488, 0.67176499, 0.66577969, 0.77855072, 0.89440569],
    [0.13782374, 0.54511152, 0.68135909, 0.35000627, 0.98777592],
    [0.67635072, 0.02870366, 0.68223704, 0.38844812, 0.14223782],
    [0.8717936, 0.65624967, 0.43774147, 0.84796056, 0.03645021],
    [0.26524294, 0.89012914, 0.68456917, 0.3762849, 0.28373839],
    [0.09760339, 0.5167722, 0.81063279, 0.67957379, 0.88101364],
    [0.08311124, 0.11833117, 0.75934227, 0.16394095, 0.98707961],
    [0.87433752, 0.73598231, 0.42740926, 0.71862831, 0.56474663],
    [0.12142517, 0.67049025, 0.95329281, 0.70491311, 0.71208761],
    [0.07701367, 0.25914836, 0.26492714, 0.11204349, 0.07689888],
    [0.75874718, 0.36944835, 0.83182752, 0.5053989, 0.86252777],
    [0.63914926, 0.14621695, 0.53266967, 0.24460243, 0.09575971],
    [0.68551271, 0.80376432, 0.76807593, 0.757307, 0.81691992],
    [0.33742753, 0.54315524, 0.89017836, 0.58244099, 0.72030092],
    [0.42430832, 0.68593713, 0.32410327, 0.94815491, 0.35854353],
    [0.20421792, 0.662545, 0.99375489, 0.1236569, 0.07012716],
    [0.6822491, 0.41628329, 0.76823939, 0.42216011, 0.06345173],
    [0.72619165, 0.36800944, 0.02790505, 0.34170873, 0.32815182],
    [0.48660647, 0.16396694, 0.53586129, 0.0549655, 0.36013182],
    [0.13428799, 0.00872681, 0.92441481, 0.76401238, 0.9263514],
    [0.07542963, 0.79419781, 0.55451018, 0.23353042, 0.21299376],
    [0.42992294, 0.50970919, 0.73278174, 0.37957002, 0.36943813],
    [0.15601848, 0.23560319, 0.81344772, 0.71416631, 0.40832455],
    [0.28538701, 0.0441685, 0.14211229, 0.84665841, 0.52547343],
    [0.49330773, 0.83516361, 0.59440407, 0.5262435, 0.38719481],
    [0.5473437, 0.85523101, 0.34450809, 0.15450294, 0.42892232],
    [0.56307153, 0.37315113, 0.51149837, 0.87285165, 0.56914953],
    [0.24476026, 0.63344091, 0.25951363, 0.50924082, 0.50695228],
    [0.69786623, 0.05742253, 0.48623967, 0.69561199, 0.1313565],
    [0.91155782, 0.6072823, 0.65620891, 0.54397315, 0.39669039],
    [0.68241761, 0.93548642, 0.70073906, 0.17176263, 0.12648921],
    [0.77985046, 0.59238734, 0.82599061, 0.99663779, 0.58755769],
    [0.02130969, 0.38961136, 0.13276389, 0.06851216, 0.83094414],
    [0.8314817, 0.42930476, 0.15523271, 0.93139498, 0.06851216]
])

# Define the window size for segmentation
window_size = 55

# Split continuous test data into segments of window_size
X_test_segments = [X_test[i:i + window_size] for i in range(0, len(X_test), window_size)]

# Placeholder for the labels
y_test_one_hot_output_1 = utils.to_categorical(np.zeros(len(X_test_segments)), num_classes=num_classes)
y_test_one_hot_output_2 = utils.to_categorical(np.zeros(len(X_test_segments)), num_classes=num_classes)
y_test_one_hot_output_3 = utils.to_categorical(np.zeros(len(X_test_segments)), num_classes=num_classes)
y_test_one_hot_output_4 = utils.to_categorical(np.zeros(len(X_test_segments)), num_classes=num_classes)
y_test_one_hot_output_5 = utils.to_categorical(np.zeros(len(X_test_segments)), num_classes=num_classes)
y_test_one_hot_output_6 = utils.to_categorical(np.zeros(len(X_test_segments)), num_classes=num_classes)
y_test_one_hot_output_7 = utils.to_categorical(np.zeros(len(X_test_segments)), num_classes=num_classes)


input_size = 5

# Define a neural network model with 7 output nodes
model = models.Sequential([
    layers.Dense(256, activation='relu', input_shape=(input_size,)),
    layers.Dense(256, activation='relu', input_shape=(input_size,)),
    layers.Dense(64, activation='relu', input_shape=(input_size,)),
    layers.Dense(64, activation='relu', input_shape=(input_size,)),
    layers.Dense(32, activation='relu'),
    layers.Dense(7, activation='softmax')  # 7 output nodes
])


# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])


# Convert X_train_features for all 7 outputs to a single array
X_train_features_combined = np.concatenate((X_train_features_1, X_train_features_2, X_train_features_3, X_train_features_4, X_train_features_5, X_train_features_6, X_train_features_7), axis=0)


# Convert y_train_one_hot for all 7 outputs to a single array
y_train_one_hot_combined = np.concatenate((y_train_one_hot_1, y_train_one_hot_2, y_train_one_hot_3, y_train_one_hot_4, y_train_one_hot_5, y_train_one_hot_6, y_train_one_hot_7),axis=0)


# Train the model with the combined features and labels
model.fit(X_train_features_combined, y_train_one_hot_combined, epochs=10, batch_size=32, validation_split=0.1)



# Evaluate the model on the test set segments
for i in range(len(X_test_segments)):
    X_test_segment = X_test_segments[i]

    predictions = model.predict(X_test_segment)
    predicted_labels = tf.argmax(predictions, axis=1).numpy()

    # Choose the output with the highest confidence for each test data segment
    chosen_output = np.argmax(np.sum(predictions, axis=0))

    print(f"Chosen output for window {i + 1}: {chosen_output}")
